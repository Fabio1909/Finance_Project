{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681954b9-9019-46c3-8aa0-ede814f3d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222c3d0-a285-4266-a023-b9b2d65135b3",
   "metadata": {},
   "source": [
    "# Processing all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c78e0b-84cb-4284-9b94-6eeab66186f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: 107440 images with shape (32, 15)\n",
      "FALSE: 85738 images with shape (32, 15)\n",
      "CPU times: user 2.23 s, sys: 6.4 s, total: 8.63 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images_True = []\n",
    "images_False = []\n",
    "image_dir = \"data/images_small/traning/5-day/\"\n",
    "dirist = [x for x in os.listdir(image_dir) if not x.startswith(\".\")]\n",
    "for folder_name in dirist:\n",
    "    folder_dir = os.path.join(image_dir, folder_name)\n",
    "    for file_name in os.listdir(folder_dir):\n",
    "        file_dir = os.path.join(folder_dir, file_name)\n",
    "        label = 1 if file_dir.strip().endswith(\"True.png\") else 0\n",
    "        img = cv2.imread(file_dir, cv2.IMREAD_GRAYSCALE)\n",
    "        if label == 1:\n",
    "            images_True.append(img)\n",
    "        else:\n",
    "            images_False.append(img)\n",
    "images_True = np.array(images_True)\n",
    "#images_True = images_True / 255.0 # --> Apparently Kelly does not scale betwen 0 and 1\n",
    "\n",
    "images_False = np.array(images_False)\n",
    "#images_False = images_False / 255.0\n",
    "\n",
    "print(f\"TRUE: {len(images_True)} images with shape {images_True[0].shape}\")\n",
    "print(f\"FALSE: {len(images_False)} images with shape {images_False[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e35ffa-5a15-4994-bb27-dffb8a46aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total memory usage of the image list is approximately 0.061 GB.\n",
      "The total memory usage of the image list is approximately 0.049 GB.\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_usage(image_list):\n",
    "    total_bytes = sys.getsizeof(image_list)  # Get the size of the list itself\n",
    "    for img in image_list:\n",
    "        total_bytes += sys.getsizeof(img)  # Add the size of each image array\n",
    "    \n",
    "    total_gb = total_bytes / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "    print(f\"The total memory usage of the image list is approximately {total_gb:.3f} GB.\")\n",
    "\n",
    "calculate_memory_usage(images_True)\n",
    "calculate_memory_usage(images_False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41782c88-d2ba-436f-8532-220286d7b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 120030 samples\n",
      "Test set: 51442 samples\n",
      "Image shape: (32, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Assign labels\n",
    "true_labels = np.ones(len(images_True))  # Label 1 for True images\n",
    "false_labels = np.zeros(len(images_False))  # Label 0 for False images\n",
    "\n",
    "# Step 1.5: Balance the dataset\n",
    "# As we have 21706 more true images than false we will remove these \n",
    "true_labels = true_labels[21706:]\n",
    "images_True = images_True[21706:]\n",
    "\n",
    "# Step 2: Combine images and labels\n",
    "images = np.concatenate([images_True, images_False], axis=0)\n",
    "labels = np.concatenate([true_labels, false_labels], axis=0)\n",
    "\n",
    "# Step 3: Shuffle and split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Image shape: {X_train[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca4b4e-bb4d-4065-8e12-668ee5815574",
   "metadata": {},
   "source": [
    "### Calculate some metris to make sure that the traning data is balances with respect to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae8c763-5182-464f-813e-3aa8d1c0a76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85734"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83753bfb-d70a-4067-9ca3-12d131579bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85738"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(false_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e6aa88-e743-4112-b52c-8e5e82847740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999916687494793"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b631d-9667-4a78-8810-b7f68bacbb5a",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e252800d-9646-42d6-99c8-585caa5cddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform  # Xavier initializer\n",
    "\n",
    "def build_cnn_model(input_shape=(32, 15)):\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(64, (5, 3), padding='same', kernel_initializer=GlorotUniform(), input_shape=(*input_shape, 1)),\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((2, 1)),  # Pooling with size (2, 1)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(128, (5, 3), padding='same', kernel_initializer=GlorotUniform()),\n",
    "        layers.BatchNormalization(),  # Batch normalization\n",
    "        layers.LeakyReLU(alpha=0.1),  # Leaky ReLU with alpha=0.1\n",
    "        layers.MaxPooling2D((2, 1)),  # Pooling with size (2, 1)\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, kernel_initializer=GlorotUniform()),  # Fully connected layer\n",
    "        layers.Dropout(0.5),  # 50% dropout\n",
    "        layers.Activation('sigmoid')  # Binary classification (True/False)\n",
    "    ])\n",
    "    \n",
    "    # Use Adam optimizer with a learning rate of 0.1\n",
    "    optimizer = Adam(learning_rate=0.1)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52beb1b-323a-4625-8a74-431162de989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 15, 64)        1024      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32, 15, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32, 15, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 15, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 15, 128)       123008    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 15, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 15, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 15, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 15360)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 15361     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1)                 0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140161 (547.50 KB)\n",
      "Trainable params: 139777 (546.00 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_cnn_model()\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f05e2b-92e0-4137-9829-b54fd37880c5",
   "metadata": {},
   "source": [
    "# Traning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291a5b9-889d-4f78-b076-e7d797a14486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EarlyStopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.3,\n",
    "    callbacks=callbacks\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd7254-0488-40c1-bdee-e4a3baacfbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e3e67-631e-4870-8477-b065144e5e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5b70e-4f54-4435-bb92-0e16c86b0c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c5667-8526-45b1-a479-dcbb12350510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27f548-d520-45ab-b319-d88623528c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb4de4-03a7-4b45-bd0e-d161988fa4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e845430-f0bf-4083-93dd-6deeea5ee1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843729b-d928-46d3-ad8e-27ae59741686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f923a88-d2d8-42af-b02c-4a510f5618d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5615b3c-2ea8-455d-9f37-e82d395472f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3889e30-aaa1-49fd-8d10-14f6492a8769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d99158-bb6a-4f70-8d0e-39bce7913d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cfac7-f28c-4c87-b64c-34e4a318377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")  # Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20744a83-a4fb-451c-b639-05537af38c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model(\"my_model.h5\")  # Load the model from the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
